# -*- coding: utf-8 -*-
from pathlib import Path
import json, time
import pandas as pd
import numpy as np
from pandas.api.types import is_datetime64_any_dtype

BASE = Path(r"D:\chan\chan.py")
DIR_KLINES   = BASE / r"D:\chan\chan.py\data\stooq\1d_15y"           # AAPL_1d.parquet
DIR_LABELED  = BASE / r"D:\chan\chan.py\events_outputs\labels"   # *_events_labeled.csv

run_tag = time.strftime("%Y%m%d_%H%M%S")
OUT_DIR = BASE / r"ml_dataset" / run_tag
OUT_DIR.mkdir(parents=True, exist_ok=True)

CHECK_TENTRY_IN_KLINE = True   # å¼€ç€æ›´å®‰å…¨ï¼Œå‡º WARNING æ–¹ä¾¿æ’æŸ¥

# ---------- 1) å»ºç«‹ K çº¿ç¬¦å·æ˜ å°„ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰ ----------
def build_kline_symbol_map():
    """
    æ‰«æ K çº¿ç›®å½•ï¼Œæ”¶é›†å½¢å¦‚ <SYMBOL>_1d.parquet é‡Œçš„ SYMBOLã€‚
    è¿”å›: {lower_symbol: ProperCaseSymbol}
    """
    m = {}
    for p in DIR_KLINES.glob("*_1d_15y.parquet"):
        stem = p.stem  # e.g., "AAPL_1d"
        sym = stem[:-7] if stem.lower().endswith("_1d_15y") else stem
        #è¿™æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ‡ç‰‡æ“ä½œã€‚[:-3] çš„æ„æ€æ˜¯â€œè·å–ä»å­—ç¬¦ä¸²å¼€å¤´åˆ°å€’æ•°ç¬¬4ä¸ªå­—ç¬¦ï¼ˆä¸å«ï¼‰çš„æ‰€æœ‰å­—ç¬¦â€ã€‚
        m[sym.lower()] = sym
    return m

SYM_MAP = build_kline_symbol_map()
if not SYM_MAP:
    raise FileNotFoundError(f"åœ¨ {DIR_KLINES} æ²¡æ‰¾åˆ° *_1d_15y.parquetï¼Œæ— æ³•å»ºç«‹ç¬¦å·æ˜ å°„ã€‚")

# ---------- 2) ä» labeled æ–‡ä»¶åé‡Œæ‹¿åˆ°â€œç¬¦å·å€™é€‰â€ ----------
def symbol_candidate_from_labeled(fp: Path) -> str:
    """
    aapl_events_labeled.csv -> 'aapl'
    AAPL_1d_events_labeled.csv -> 'AAPL' ï¼ˆä¼šæŠŠå°¾éƒ¨ _1d åˆ‡æ‰ï¼‰
    """
    s = fp.stem  # ä¾‹å¦‚ "aapl_events_labeled"
    s_low = s.lower()
    if s_low.endswith("_events_labeled"):
        s = s[: -len("_events_labeled")]
    if s.lower().endswith("_1d"):  # å…¼å®¹æœ‰äººå¸¦äº† _1d
        s = s[:-3]
    return s.strip()

def resolve_symbol(cand: str) -> str | None:
    """
    cand åœ¨ K çº¿æ˜ å°„é‡Œåˆ™è¿”å›è§„èŒƒå†™æ³•ï¼Œå¦åˆ™ None
    """
    return SYM_MAP.get(cand.lower())#å½“ä¼ å…¥çš„ cand å‚æ•°ï¼ˆè½¬æ¢ä¸ºå°å†™åï¼‰åœ¨ SYM_MAP å­—å…¸ä¸­ä¸å­˜åœ¨æ—¶ï¼Œ.get() æ–¹æ³•ä¼šé»˜è®¤è¿”å› Noneã€‚

# ---------- 3) è¯» K çº¿ä¸º UTC ç´¢å¼• ----------
def load_klines(symbol: str, timeframe: str = "1d_15y") -> pd.DataFrame:
    f = DIR_KLINES / f"{symbol}_{timeframe}.parquet"
    if not f.exists():
        return None  # äº¤ç»™ä¸Šæ¸¸åˆ¤æ–­æ˜¯å¦è·³è¿‡
    df = pd.read_parquet(f)

    if is_datetime64_any_dtype(df.index):
        idx = pd.DatetimeIndex(df.index)
        df.index = idx.tz_localize("UTC") if idx.tz is None else idx.tz_convert("UTC")
        return df.sort_index()

    for cand in ["timestamp", "datetime", "date", "time"]:
        if cand in df.columns:
            ts = pd.to_datetime(df[cand], utc=True, errors="coerce")
            if ts.isna().any():
                raise ValueError(f"{f} çš„ {cand} åˆ—æœ‰æ— æ³•è§£æçš„æ—¶é—´æˆ³ã€‚")
            df = df.set_index(ts).drop(columns=[cand])
            return df.sort_index()

    raise ValueError(f"{f} ç¼ºå°‘å¯ç”¨çš„æ—¶é—´åˆ—ï¼Œä¸”ç´¢å¼•ä¹Ÿä¸æ˜¯æ—¶é—´ç±»å‹ã€‚")

# ---------- 4) è¯» labeled å¹¶æ ‡å‡†åŒ– ----------
def read_and_standardize_labeled(fp: Path, symbol: str, timeframe: str) -> pd.DataFrame:
    df = pd.read_csv(fp)

    # t_entry_tsï¼šç»Ÿä¸€ä¸º UTC
    if "t_entry_ts" in df.columns:
        ts = pd.to_datetime(df["t_entry_ts"], utc=True, errors="coerce")
    elif "t_entry" in df.columns:
        ts = pd.to_datetime(df["t_entry"], utc=True, errors="coerce")
    else:
        raise KeyError(f"{fp.name} ç¼ºå°‘ t_entry æˆ– t_entry_ts åˆ—ã€‚")
    if ts.isna().any():
        raise ValueError(f"{fp.name} ä¸­æœ‰ {int(ts.isna().sum())} æ¡æ—¶é—´æˆ³æ— æ³•è§£æã€‚")
    df["t_entry_ts"] = ts

    # å¿…éœ€åˆ—
    for c in ["entry_price", "direction", "y", "ret"]:
        if c not in df.columns:
            raise KeyError(f"{fp.name} ç¼ºå°‘å¿…éœ€åˆ—: {c}")
    df["direction"] = pd.to_numeric(df["direction"], errors="coerce").astype("Int64")
    df["y"]         = pd.to_numeric(df["y"], errors="coerce").astype("Int64")
    df["ret"]       = pd.to_numeric(df["ret"], errors="coerce")

    # event_id
    if "event_id" not in df.columns or df["event_id"].isna().any():
        df["event_id"] = [f"{symbol}_1d_{t.isoformat()}" for t in df["t_entry_ts"]]

    df["symbol"]    = symbol
    df["timeframe"] = timeframe

    keep = ["event_id","symbol","timeframe","t_entry_ts","entry_price","direction","y","ret"]
    for extra in ["exit_reason","bars_held","exit_loc","k","m","H","fee_pct","tie"]:
        if extra in df.columns: keep.append(extra)
    out = df[keep].copy().sort_values("t_entry_ts").reset_index(drop=True)

    # å¯é€‰æ£€æŸ¥ï¼št_entry_ts æ˜¯å¦åœ¨è¯¥æ ‡çš„ K çº¿é‡Œ
    if CHECK_TENTRY_IN_KLINE:
        kl = load_klines(symbol, "1d_15y")
        if kl is None:
            print(f"âš ï¸  {symbol}: ç¼ºå°‘ K çº¿æ–‡ä»¶ï¼Œè·³è¿‡ç´¢å¼•æ£€æŸ¥ã€‚")
        else:
            missing = (~out["t_entry_ts"].isin(kl.index)).sum()
            if missing > 0:
                print(f"âš ï¸  {fp.name}: æœ‰ {int(missing)} æ¡ t_entry_ts ä¸åœ¨ {symbol}_1d çš„Kçº¿ç´¢å¼•ä¸­ï¼ˆç²’åº¦/æ—¶åŒºä¸ä¸€è‡´æˆ–äº‹ä»¶è½åœ¨éäº¤æ˜“æ—¥ï¼‰ã€‚")

    return out

# ---------- 5) ä¸»æµç¨‹ ----------
def main():
    files = sorted(DIR_LABELED.glob("*_events_labeled.csv"))
    if not files:
        raise FileNotFoundError(f"åœ¨ {DIR_LABELED} æœªæ‰¾åˆ° *_events_labeled.csv")

    pieces, skipped = [], []

    print(f"ğŸ“¦ Kçº¿å¯ç”¨æ ‡çš„æ•°: {len(SYM_MAP)}ï¼Œç¤ºä¾‹: {list(SYM_MAP.values())[:5]}")

    for fp in files:
        cand = symbol_candidate_from_labeled(fp)
        symbol = resolve_symbol(cand)
        if symbol is None:
            skipped.append((fp.name, cand, "no_matching_kline_file"))
            print(f"â­ï¸  SKIP {fp.name}: è§£æåˆ°ç¬¦å·å€™é€‰ '{cand}'ï¼Œä½†åœ¨ {DIR_KLINES} æ‰¾ä¸åˆ°å¯¹åº” *_1d_15y.parquetã€‚")
            continue

        # å†æ¬¡ç¡®è®¤ Kçº¿æ˜¯å¦å­˜åœ¨ï¼ˆé˜²å¾¡ï¼‰
        kl_file = DIR_KLINES / f"{symbol}_1d_15y.parquet"
        if not kl_file.exists():
            skipped.append((fp.name, symbol, "kline_parquet_missing"))
            print(f"â­ï¸  SKIP {fp.name}: '{symbol}_1d_15y.parquet' ä¸å­˜åœ¨ã€‚")
            continue

        
        df_one = read_and_standardize_labeled(fp, symbol, "1d")
        pieces.append(df_one)

    if not pieces:
        raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„ labeled æ–‡ä»¶è¢«å¤„ç†ã€‚è¯·æ£€æŸ¥å‘½åæˆ–ç›®å½•å†…å®¹ã€‚")

    dataset = pd.concat(pieces, ignore_index=True)
    dataset = dataset[dataset["t_entry_ts"].notna()]
    dataset = dataset[dataset["y"].isin([0,1])]
    dataset = dataset.drop_duplicates(subset=["event_id"]).sort_values("t_entry_ts").reset_index(drop=True)

    # 70/15/15 æ—¶é—´åˆ‡åˆ†
    #è®­ç»ƒé›†(70%)ï¼šç”¨äºæ¨¡å‹è®­ç»ƒï¼Œå­¦ä¹ å†å² patterns
    #éªŒè¯é›†(15%)ï¼šç”¨äºè°ƒå‚å’Œæ¨¡å‹é€‰æ‹©ï¼Œæ¨¡æ‹Ÿ"è¿‘æœŸè¿‡å»"
    #æµ‹è¯•é›†(15%)ï¼šç”¨äºæœ€ç»ˆè¯„ä¼°ï¼Œæ¨¡æ‹Ÿ"æœªçŸ¥æœªæ¥"
    q70 = dataset["t_entry_ts"].quantile(0.70)
    q85 = dataset["t_entry_ts"].quantile(0.85)
    train = dataset[dataset["t_entry_ts"] <= q70].copy()
    valid = dataset[(dataset["t_entry_ts"] > q70) & (dataset["t_entry_ts"] <= q85)].copy()
    test  = dataset[dataset["t_entry_ts"] > q85].copy()

    # å¯¼å‡º
    dataset.to_parquet(OUT_DIR / "dataset_all.parquet", index=False)
    train.to_parquet(OUT_DIR / "train.parquet", index=False)
    valid.to_parquet(OUT_DIR / "valid.parquet", index=False)
    test.to_parquet(OUT_DIR / "test.parquet", index=False)

    manifest = {
        "run_tag": run_tag,
        "paths": {
            "all":   str(OUT_DIR / "dataset_all.parquet"),
            "train": str(OUT_DIR / "train.parquet"),
            "valid": str(OUT_DIR / "valid.parquet"),
            "test":  str(OUT_DIR / "test.parquet"),
        },
        "counts": {
            "all": len(dataset),
            "train": len(train),
            "valid": len(valid),
            "test":  len(test),
        },
        "positives_ratio": {
            "all": float(dataset["y"].mean()) if len(dataset) else None,
            "train": float(train["y"].mean()) if len(train) else None,
            "valid": float(valid["y"].mean()) if len(valid) else None,
            "test":  float(test["y"].mean())  if len(test)  else None,
        },
        "by_symbol_counts": dataset.groupby("symbol")["event_id"].count().sort_values(ascending=False).to_dict(),
        "columns": dataset.dtypes.astype(str).to_dict(),
        "skipped_files": skipped,
        "note": "äº‹ä»¶æ—¶é—´ç»Ÿä¸€ä¸º UTCï¼›ä»…åˆå¹¶èƒ½åœ¨Kçº¿ç›®å½•ä¸­æ‰¾åˆ°çš„æ ‡çš„ã€‚",
    }
    with open(OUT_DIR / "manifest.json", "w", encoding="utf-8") as f:
        json.dump(manifest, f, ensure_ascii=False, indent=2)

    print("\nğŸ“ è¾“å‡ºç›®å½•ï¼š", OUT_DIR)
    print(json.dumps(manifest, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()
